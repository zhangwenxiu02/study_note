# FedEgo: Privacy-preserving Personalized Federated Graph Learning with Ego-graphs

## Q:论文是做什么的，摘要第一段

图作为包含结构和特征信息的特殊信息载体，广泛应用于图挖掘，例如图神经网络（GNN）。

- 在一些实际场景中，图数据分别存储在多个分布式各方中，由于利益冲突，可能无法直接共享。因此，提出了联合图神经网络来解决此类数据孤岛问题，同时保护各方（或客户）的隐私。
- 各方之间不同的图数据分布（称为统计异质性）可能会降低 FedAvg 等简单联邦学习算法的性能。在本文中，我们提出了 FedEgo，一个基于自我图的联合图学习框架，以应对上述挑战。
- 每个客户将训练他们的本地模型，同时也为全局模型的训练做出贡献。  GraphSAGE 应用于自我图，以充分利用结构信息，并利用 Mixup 来解决隐私问题。为了处理统计异质性，我们将个性化融入学习中，并提出**自适应混合系数策略**，使客户能够实现最佳个性化。大量的实验结果和深入的分析证明了FedEgo的有效性。

## Q: 什么是FedEgo模型

![image-20240306231400204](C:\Users\张文秀zwx\AppData\Roaming\Typora\typora-user-images\image-20240306231400204.png)

1. **初始化（Require部分）**：
    - 客户端数量 \( N \)
    - 节点特征矩阵 \( \{X_i\} \)
    - 减少层的初始权重 \( \Theta_i \)
    - 个性化层的初始权重 \( \phi_i \)
    - 全局模型的个性化层初始权重 \( \phi_g \)

2. **客户端操作（Local Stage）**：
    - 每个客户端采样 \( k \) 个ego-graph（一个节点及其邻居的局部图结构），并计算局部分布向量 \( p \)。
    - 在本地阶段，每个客户端并行执行以下操作：
        - 对每个批次数据进行循环。
        - 对批次中的每个节点，计算其特征 \( x_o \) 的局部减少层 \( r_o \) 和个性化层 \( p_o \) 的输出。
        - 生成一个合成的ego-graph，并为ego-graph中的每个节点计算嵌入和标签。
        - 更新参数，通过损失 \( l_i \) 来调整 \( \Theta_i \) 和 \( \phi_i \)。

3. **全局阶段（Global Stage）**：
    - 客户端发送减少层的权重 \( \Theta_i \) 到服务器。
    - 服务器通过平均这些权重来更新全局参数 \( \Theta_g \)。
    - 客户端发送合成的ego-graph的嵌入和标签到服务器。
    - 服务器对全局模型的个性化层进行训练。
    - 更新全局参数 \( \phi_g \)。

4. **客户端的参数更新（Parameters update for clients）**：
    - 服务器计算全局分布向量 \( p_g \)。
    - 对每个客户端并行执行以下操作：
        - 计算EMD（Earth Mover's Distance，一种测量两个概率分布差异的方法）和 \( \lambda_i \)。
        - 根据EMD和 \( \lambda_i \) 更新 \( \Theta_i \) 和 \( \phi_i \)。

## Q：整体模型训练流程

### 本地阶段 (Local Stage)

1. **减少层**：客户端将本地的ego-graphs送入减少层（使用多层感知机MLP构建），以获得低维度的嵌入。这一步骤旨在捕获不同客户端数据之间可能共享的低维表示。

2. **Mixup过程**：接下来，在每个批次内，通过Mixup过程生成mashed ego-graph。这一步增加了数据的多样性，同时也增强了隐私保护。

3. **个性化层**：然后，将从减少层获得的嵌入进一步送入个性化层（使用GraphSAGE网络架构），以进行个性化图挖掘。

4. **参数更新**：每个客户端计算损失并更新减少层和个性化层中的参数。

### 全局阶段 (Global Stage)

1. **参数上传**：客户端上传减少层的参数和生成的mashed ego-graphs到服务器，以便协作。

2. **参数聚合**：服务器使用FedAvg算法聚合上传的减少层参数，并通过在mashed ego-graphs上训练来更新全局的个性化层。

3. **参数反馈**：更新后的所有参数被发送回客户端。客户端随后加载减少层参数，并通过混合本地和全局权重来更新其个性化层。

## Q:解决了什么挑战

 • 挑战一：如何在联邦训练中充分利用图数据的结构信息？由于拓扑信息是图挖掘不可或缺的一部分，因此将它们有机地融入联邦学习中非常重要。

可以将图视为具有结构和节点特征的 k 跳自我图族。自我图是中心节点最多k跳邻居的采样子图，是一种有用的信息载体，可以在图挖掘过程中实现结构和特征信息的消息传递。 FedEgo 通过在自我图上应用 GraphSAGE [9] 来提取拓扑信息。此外，从采样的自我图中只能得出局部拓扑信息，不可能恢复原始结构，这意味着它们是结构匿名的。

 • 挑战2：如何缓解联邦学习框架中潜在的非独立同分布图数据的问题？从不同角度观察真实数据集会导致严重的非独立同分布图数据，并导致朴素的 FL 算法无法正常运行。 

为了解决挑战 2，我们在中央服务器中训练一个全局模型来处理非 IID 图数据。考虑到挑战 1，我们寻求开发一个能够捕获自我图包含的结构信息的全局模型。为此，受[1, 7]的启发，我们引入了一个由约简层和个性化层组成的网络，分别利用共享的低维嵌入和执行个性化图挖掘。在 FedEgo 中，客户端的模型由用于本地训练的还原层和个性化层组成，而服务器中的全局模型仅由用于全局数据集中自我图的信息蒸馏的个性化层组成。通过在客户端的每个批次中应用 Mixup [23, 24]，生成混合的自我图，随后将其发送到服务器并形成全局数据集。在服务器中，全局模型处理非独立同分布图数据的能力是通过对上传的混合自我图进行训练来开发的。混合的自我图仅包含混合的嵌入和局部结构，防止原始数据的传输并保护隐私，因此它们进一步是特征匿名的。

• 挑战3：如何在协作的利弊之间实现最佳权衡？在潜在的非独立同分布场景下，客户的理想情况是利用其他人的数据来补偿其本地数据集，同时最大限度地减少彼此之间统计异质性造成的损害。

客户端将在还原层中遵循 FedAvg [15] 的普通算法，以在加密自我图方面达成共识，并且然后根据自适应混合系数在个性化层中混合局部和全局权重。混合系数有助于为每个客户实现更好的个性化，它是根据本地数据集和全局数据集的分布之间的差异自适应确定的。

## Q:作者的核心贡献（典型三段式）

- 我们介绍了一种基于自我图的新型个性化联合图学习框架。 FedEgo 通过在自我图上应用 GraphSAGE 来充分利用图数据的结构信息。 
- 出于隐私问题，将 Mixup 应用于自我图，并通过对上传的混合自我图进行训练，开发全局模型捕获结构信息和处理非独立同分布图数据的能力。 
- 我们设计了一种策略来自适应地学习个性化模型，以在协作的利弊之间进行权衡。 
- 我们对广泛使用的数据集进行了广泛的实验，并凭经验证明了FedEgo 在非独立同分布场景下的卓越性能。

## Q：GraphSAGE是什么？

- 该模型每次训练和预测时只使用整张图（所有节点）中的少量部分节点，训练出来的模型对新增节点的预测效果较好，因此更具实用性。核心思想在于学习如何聚合来自目标节点邻域的特征信息，来生成目标节点的嵌入表示。

1. **邻居采样**：对于每个目标节点，从其邻居中随机采样一个固定大小的集合。这样做可以减少计算复杂性，并使得每个节点在每一层的邻居数量保持一致。

2. **聚合邻居信息**：使用聚合函数（如平均、池化等）来汇总邻居的特征信息。GraphSAGE提出了几种不同类型的聚合函数。

3. **更新节点表示**：将聚合后的邻居信息与目标节点自身的特征结合起来，通常是通过连接操作或者加权和来完成。然后，使用非线性激活函数来更新节点表示。

4. **多层结构**：重复以上步骤来构建多层的GraphSAGE网络。每一层都聚合来自上一层的邻居信息，逐步扩展节点的邻域。

在多层GraphSAGE网络中，“每一层”指的是网络中的一层，每层都对应着一次邻居信息的聚合过程。在每一层，每个节点都会生成一个新的嵌入向量，这个向量是基于其邻居（在前一层中已经被聚合的节点）的信息。

GraphSAGE特别适用于节点分类和链接预测等下游任务，因为它生成的节点嵌入向量可以捕捉到节点的局部图结构信息以及节点特征。通过使用GraphSAGE，我们可以为每个节点生成一个表征其在图中上下文的嵌入向量。

## Q:如何建图？

在FedEgo框架中，自我图（ego-graph）是这样构建的：

- **自我图的定义**：自我图是以特定节点为中心的子图，包含该节点以及与其相连的k阶邻居（k-hop neighbors）。这意味着，如果一个节点与中心节点之间的最短路径长度不超过k，则该节点会被包含在自我图中。

- **构建过程**：FedEgo中，每个客户端为训练采样自我图。在实践中，采样是跟随GraphSAGE方法，为每个节点的每一层采样固定大小的邻居集合。这样采样的自我图具有固定的形状，并且可以独立于原始图进行训练。

- **特点**：训练时，并不关心原始图中的具体信息，只关心局部属性。因此，无法通过采样的自我图恢复原始图的完整结构，这保证了采样的自我图在结构上是匿名的，有利于保护隐私。

- **隐私保护**：FedEgo利用自我图的这一特性，结合图神经网络的优点，实现了隐私保护的个性化联邦图学习。通过自我图，可以捕获节点的局部结构信息并用于图数据的深度学习，而不需要将整个数据集集中在一处，从而保证了数据的隐私性。

## Q：什么叫为每个节点的每一层采样固定大小的邻居集合。

在GraphSAGE和类似的图神经网络方法中，图的每个节点都通过多层的方式来聚合信息。"为每个节点的每一层采样固定大小的邻居集合"意味着在这个多层处理过程中，对于每个节点，我们在每一层上都会选择固定数量的邻居来进行信息聚合。

这里的"层"（layer）指的是神经网络中的概念。在图神经网络中，层与层之间的连接表示的是节点与它的邻居之间的关系。对于每个节点，我们首先看它的直接邻居（第1层邻居），然后是邻居的邻居（第2层邻居），以此类推。信息会从节点的远处邻居聚合到更近的邻居，最后聚合到节点本身。

GraphSAGE采用的是一种称为邻居采样（neighbor sampling）的方法。在每一层，为了减少计算复杂性和避免整个图的计算，我们不使用节点的所有邻居进行信息聚合，而是随机选择一定数量（固定大小）的邻居。这种采样策略允许网络在处理大规模图数据时保持可扩展性，因为它避免了使用完整邻居集合的昂贵计算。

这种方法特别适用于不能直接处理整个图的情况，如因为隐私问题分布在多个客户端的数据集。通过采样，每个节点可以仅使用其邻居的部分信息来更新其嵌入，这有助于在保护数据隐私的同时，使模型能够学习到图的局部结构信息。

## Q：mixup怎么使用？

Mixup是一种数据增强技术，通常用于传统的深度学习中，用于训练分类器对输入数据的插值进行鲁棒性建模。Mixup的核心思想是创建虚拟的训练样本，通过以下方式：

1. 对于两个输入样本的特征向量 \( x_i, x_j \)，Mixup会创建一个新的特征向量 $ x_{mix}$
   $$ x_{mix} = \lambda x_i + (1 - \lambda) x_j $$
   其中 \( \lambda \) 是从 [0,1] 区间中随机采样的。
2. 对于这两个样本对应的标签 \( y_i, y_j \)，也会以同样的方式混合：
   $$y_{mix} = \lambda y_i + (1 - \lambda) y_j $$

这样，模型就会学习如何在这些混合样本上进行泛化，而不是仅仅在原始训练数据上。

在图数据和联邦学习环境中，直接应用Mixup可能存在挑战，特别是在处理非独立同分布（Non-IID）的数据时。在联邦图学习框架FedEgo中，客户端生成的是结构匿名的mashed ego-graphs，用于训练全局模型，而不是原始的图数据。这些mashed ego-graphs在每个批次中通过Mixup生成，并且发送到服务器。但在图数据中，由于结构的不规则性，不能直接进行逐元素的Mixup操作。

在FedEgo中，实现Mixup时采取的方法是：

1. 将每个批次中的ego-graphs的中心节点对齐。
2. 递归地扩展它们的邻居，为每个节点在其相应层中分配一个特定位置。
3. 根据对齐，简单地平均相应节点的嵌入，从而获得mashed ego-graph中每个位置的嵌入。
4. 这种方法生成的mashed ego-graphs平均了缩减

嵌入（即通过降维层得到的嵌入），以此来防止隐私泄露，同时也保持了特征匿名。这样的处理使得服务器无法得知

Mashed ego-graphs在FedEgo框架中通过特定的方式进行mixup处理，其目的是在每个批次中创建结构匿名的图形数据，这些数据随后发送给服务器而不是原始数据。这种做法可以防止服务器知道原始图的结构以及某个特定节点是否存在于客户端的本地数据集中。

由于图数据的结构不规则性，传统数据的Mixup方法（直接在样本特征之间进行插值）并不直接适用于图数据。因此，为了在固定形状的ego-graphs中应用Mixup，需要解决对齐问题。这里的“对齐”指的是，首先将同一个批次中所有ego-graphs的中心节点对齐在一起，然后递归地扩展它们的邻居节点，为每个节点在其对应层分配一个特定位置。一旦完成对齐，就可以通过简单地平均对齐后对应节点的嵌入来获得mashed ego-graph中每个位置的嵌入。这样得到的mashed ego-graph平均了减少维度的嵌入以防止隐私泄露，使其在特征上也保持匿名。

总结来说，Mashed ego-graphs的生成过程如下：

1. 对齐：将同一批次中所有ego-graphs的中心节点对齐。
2. 扩展邻居：递归地为每个节点在其对应的层中分配特定位置。
3. 平均嵌入：通过平均对应节点的嵌入来获得mashed ego-graph的嵌入。
4. 结构匿名性：确保mashed ego-graphs在结构上保持匿名，防止原始图结构的泄露。
5. 特征匿名性：通过平均嵌入来防止特征信息泄露。

这种方法确保了在不暴露个别数据点的隐私的情况下，在不同客户端之间共享学习的成果。

## Q：中心对上传的自我图要如何处理？

在FedEgo框架的全局阶段中，中心服务器处理每个客户端上传的混合自我图（mashed ego-graphs）的方法如下：

1. **接收参数和混合自我图**：客户端在训练过程的本地阶段完成后，会上传减少层的参数（reduction layers parameters）和生成的混合自我图到服务器。

2. **参数聚合**：服务器使用FedAvg算法（联邦平均算法）对上传的减少层参数进行聚合，以更新全局模型的个性化层（personalization layers）。

3. **在混合自我图上进行训练**：服务器利用聚合后的参数和收到的混合自我图来训练全局模型的个性化层。这一过程旨在捕获来自不同客户端的非独立同分布（non-IID）图数据的结构和特征信息，从而提高全局模型的泛化能力和个性化性能。

4. **更新并反馈参数**：训练完成后，服务器将更新后的全局模型参数发送回客户端。客户端根据这些参数更新其本地模型，包括减少层和个性化层的参数，以实现更好的个性化和模型性能。

通过这种方法，FedEgo能够在保护隐私的前提下，有效地处理非IID图数据，并利用联邦学习框架中的客户端协作来提升全局模型的性能。混合自我图的使用进一步增强了数据的隐私保护，因为服务器无法从这些混合图中识别出原始图的结构或特定节点的存在。

## Q:个性化层，减少层

- 充分利用联邦学习环境下的数据多样性和分布的异质性。通过在本地客户端上应用减少层和个性化层，以及在全局模型中更新个性化层，FedEgo既能够保护数据隐私，又能够提高模型的泛化能力和个性化性能。

1. **减少层**：这些层使用多层感知器（Multi-Layer Perception, MLP）构建，旨在从客户端分散的、非独立同分布（Non-IID）的数据中提取共享的低维表示。减少层处理的**核心目的**是挖掘不同客户端数据间可能共有的表示形式，即使这些数据具有不同的标签或属性。这样的处理有助于应对中央模型训练的困难，因为这些异质数据可能共享某种共同的低维嵌入，从而促进不同客户端间的协作和知识共享。

2. **个性化层**：这些层采用GraphSAGE网络架构，目的是进行个性化的图挖掘，将减少层得到的低维嵌入作为输入。个性化层的设计**旨在为每个节点**（在联邦学习环境中的客户端数据中的节点）提供定制化的嵌入，这有助于提高模型对于特定任务（如节点分类）的性能。

   通过在全局阶段训练这些层，FedEgo允许服务器捕获非IID图数据的结构和特征信息，从而为客户端更新其本地模型提供了个性化的支持。

## Q:如何理解这个代码

```
算法模型部分代码
```

结论：输入是，输出是，做了什么，做了信息交流

- 找模型limitations可以问gpt

- 找缺点，如何解决可能为创新点？

- experiment不用特别仔细看