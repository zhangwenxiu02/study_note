GAN

https://dl.acm.org/doi/10.1145/3503161.3548206

基于语言的迭代图像处理旨在根据用户的语言指令逐步编辑图像。现有的方法大多侧重于将新添加的视觉元素的属性和外观与当前指令保持一致。然而，随着迭代轮数的增加，这些方法无法保持指令与图像之间的一致性。为解决这一问题，我们提出了一种新颖的长短期一致性推理生成对抗网络（LS-GAN），它能增强对当前指令与先前对象的感知，并在不断迭代的情况下更好地保持与用户意图的一致性。具体来说，我们首先设计了一个上下文感知短语编码器（CPE），通过提取指令的不同短语级信息来学习用户意图。此外，我们还引入了长短期一致性推理（LSCR）机制。长期推理改进了模型的语义理解和位置推理能力，而短期推理则确保了根据语言指令构建视觉场景的能力。大量结果表明，LS-GAN 在对象识别和位置方面都提高了生成质量，并在两个公开数据集上取得了最先进的性能。

key words：terative instruction-based image manipulation, GAN, Long andshort term consistency reasoning



# GAN-based matrix factorization for recommender systems

https://dl.acm.org/doi/10.1145/3477314.3507099

生成对抗网络（GAN）于 2014 年提出，引发了人们对生成建模的新兴趣。它们立即在图像合成、图像到图像翻译、文本到图像生成、图像绘制等方面达到了最先进的水平，并被应用于从医学到高能粒子物理学等各种科学领域。尽管 GAN 广受欢迎，并具有学习任意分布的能力，但它尚未被广泛应用于推荐系统（RS）。 在这项工作中，我们提出了一种基于 GAN 的新方法，该方法在矩阵因式分解设置中学习用户和项目的潜在因素，以解决通用的 top-N 推荐问题。按照 CFGAN 为 RS 引入的向量式 GAN 训练方法，我们发现了将 GAN 用于 CF 时的两个独特问题。我们通过 RS 界的著名数据集对我们的模型 GANMF 进行了评估，结果表明它比传统的 CF 方法和基于 GAN 的模型都有所改进。通过对 GANMF 组件的消融研究，我们旨在了解我们的架构选择所产生的影响。最后，我们对 GANMF 的矩阵因式分解性能进行了定性评估。

关键词：collaborative filtering, matrix factorization, generative adversarialnetworks, autoencoder, feature matching

协同过滤、矩阵因式分解、生成式对抗网络、自动编码器、特征匹配

# A neuromorphic GAN system for intelligent computing on edge

https://dl.acm.org/doi/10.1145/3318216.3363334

最近，人工智能在向边缘设备发展方面取得了巨大成功。然而，涉及深度神经网络（DNN）的计算极其耗费资源和电力，成为边缘计算的一大挑战。基于可编程 ReRAM 的神经形态引擎为高效的 DNN 计算提供了机会；然而，在当前的研究中，训练过程中的内存利用率和通信延迟仍然是重大挑战，而且尚未得到解决。这一问题在具有复杂训练过程的 DNN 中变得更加严重，例如在边缘智能计算中被广泛采用的生成式对抗网络（GAN）。在这项工作中，我们在 ReRAM 神经形态引擎上设计了一个高效的 GAN 计算系统，采用在线训练框架、优化的后向计算和交叉并行计算流来高效执行训练过程，从而解决了这些难题。我们对系统性能进行了评估，并与传统的 GPU 加速器进行了比较，结果表明系统速度提高了 2.8 倍，能耗降低了 6.1 倍。

# PAR-GAN: Improving the Generalization of Generative Adversarial Networks Against Membership Inference Attacks

最近的研究表明，生成对抗网络（GAN）的泛化能力可能很差，因此容易受到隐私攻击。在本文中，我们试图从隐私保护的角度提高生成对抗网络的泛化能力，特别是在防御成员推理攻击（MIA）方面，MIA 的目的是推断特定样本是否用于模型训练。我们设计了一种 GAN 框架，即分区 GAN（PAR-GAN），它由一个生成器和多个在训练数据的不连续分区上训练过的判别器组成。PAR-GAN 的主要思想是通过近似训练数据所有分区的混合分布来缩小泛化差距。我们的理论分析表明，PAR-GAN 可以像原始 GAN 一样实现全局最优。我们在模拟数据和多个流行数据集上的实验结果表明，PAR-GAN 可以提高 GAN 的泛化能力，同时减少 MIA 引起的信息泄漏。适用于联邦学习

https://dl.acm.org/doi/epdf/10.1145/3447548.3467445

- 哈工大的

关键词：Generative Adversarial Networks, Membership Inference Attack,Generalization Gap生成式对抗网络、成员推理攻击、泛化差距

# ==T. Miyato, A. M. Dai, and I. Goodfellow, “Adversarial training methods for semi-supervised text classification,” 2017对抗扰动文献==

# Distributed Learning based on Asynchronized Discriminator GAN for remote sensing image segmentation

DGAN由多个分布式判别器和一个中央生成器组成，仅使用DGAN生成的合成遥感图像来训练语义分割模型。基于DGAN，我们建立了一个由多个不同主机组成的实验平台，该平台采用Socket和多进程技术实现主机间的异步通信，并将训练和测试过程可视化。在 DGAN 训练过程中，节点之间交换的不是原始遥感图像或卷积网络模型信息，而是合成图像、损失和标记图像。因此，DGAN 很好地保护了原始遥感图像的隐私和安全。我们在三个遥感图像数据集（City-OSM、WHU 和 KaggleShip）上验证了 DGAN 的性能。在实验中，我们考虑了遥感图像在客户端节点中的不同分布。实验结果表明，DGAN 在不共享原始遥感图像或卷积网络模型的情况下，具有很强的分布式遥感图像学习能力。此外，与在所有客户端节点收集的所有遥感图像上训练的集中式 GAN 相比，DGAN 在遥感图像的语义分割任务中几乎可以达到相同的性能。

[Distributed Learning based on Asynchronized Discriminator GAN for remote sensing image segmentation (acm.org)](https://dl.acm.org/doi/epdf/10.1145/3571662.3571668)

DGAN、遥感图像、隐私

# ==FedCG: Leverage Conditional GAN for Protecting Privacy and Maintaining Competitive Performance in Federated Learning==

联合学习（FL）旨在保护数据隐私，使客户能够在不共享其私人数据的情况下合作建立机器学习模型。最近的研究表明，在联合学习过程中交换的信息会受到基于梯度的隐私攻击，因此，人们采用了多种隐私保护方法来挫败这种攻击。然而，这些防御方法要么会带来数量级的计算和通信开销（如同态加密），要么会在预测准确性方面造成巨大的模型性能损失（如差分隐私）。在这项工作中，我们提出了一种新颖的联合学习方法 FEDCG，它利用条件生成对抗网络来实现高级别的隐私保护，同时还能保持有竞争力的模型性能。FEDCG 将每个客户端的本地网络分解为一个私有提取器和一个公共分类器，并将提取器保持在本地以保护隐私。FEDCG 不公开提取器，而是与服务器共享客户生成器，以聚合客户的共享知识，从而提高每个客户本地网络的性能。广泛的实验证明，与 FL 基线相比，FEDCG 可以获得有竞争力的模型性能，隐私分析表明 FEDCG 具有高级别的隐私保护能力。

https://www.ijcai.org/proceedings/2022/324

可信联邦学习，横向联邦学习中通过将生成对抗网络与分割学习相结合，在保护数据隐私的同时保持有竞争力的模型性能。

# Unlearning Protected User Attributes in Recommendations with Adversarial Training

[Unlearning Protected User Attributes in Recommendations with Adversarial Training | Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval](https://dl.acm.org/doi/abs/10.1145/3477495.3531820)

这些编码偏差会影响推荐系统（RS）的决策，使其进一步将提供给不同人群的内容分离开来，并引发有关泄露用户受保护属性的隐私问题。在这项工作中，我们研究了在保持推荐系统效率的同时，从推荐系统算法的交互表征中移除用户特定受保护信息的可能性和挑战。具体来说，我们在最先进的多VAE架构中加入了对抗训练，从而产生了一种新型模型--多项式似然对抗变异自动编码器（Adversarial Variational Auto-Encoder with Multinomial Likelihood，Adv-MultVAE），其目的是在保持推荐性能的同时去除受保护属性的隐含信息。我们在 MovieLens-1M 和 LFM-2b-DemoBias 数据集上进行了实验，并根据外部攻击者无法从模型中泄露用户性别信息的情况，评估了偏差缓解方法的有效性。结果表明，Adv-MultVAE 与基线 MultVAE 相比，在性能（与 NDCG 和 Recall 相比）略有下降的情况下，在很大程度上减轻了模型在这两个数据集上的固有偏差。

recommendation, adversarial training, gender bias, bias mitigation

[Adv-MultVAE：基于对抗学习的隐私保护推荐算法 - 隐私保护新闻推荐 - 新闻推荐 - 论文精读 | hang shun = 航 順 = 天官赐福，百无禁忌 (gitee.io)](https://jiang-hs.gitee.io/posts/b2c2f458/)

# Learning Fair Representations for Recommendation: A Graph-based Perspective

CCF-A

论文有两个要点，其一是**使用生成对抗网络(GAN)训练的滤波器对原始的用户－物品embeddings向量进行转换，以除去用户的敏感信息**(该论文假定原始嵌入算法不可修改，只能在已经生成的embeddings向量上做转换)；其二是**在GAN的优化目标函数(被称为价值函数)中加入用户－物品二分图的信息，以充分利用用户和物品的关系**。除此之外，该论文通篇都散发着**表示学习（representation learning）[2]** 思想的光辉。

作为人工智能的一项重要应用，推荐系统是最普遍的计算机辅助系统之一，可帮助用户找到潜在的兴趣项目。最近，研究人员对人工智能应用的公平性问题给予了极大关注。这些方法大多假定了实例的独立性，并设计了复杂的模型来消除敏感信息，以促进公平性。然而，推荐系统与这些方法有很大不同，因为用户和项目自然形成了用户-项目双向图，并且在图结构中具有协作相关性。在本文中，我们提出了一种基于图的新技术，用于确保任何推荐模型的公平性。这里的公平性要求是指在用户建模过程中不暴露敏感特征集。具体来说，给定任何推荐模型的原始嵌入，我们学习过滤器的组合，根据敏感特征集将每个用户和每个项目的原始嵌入转换为过滤后的嵌入空间。对于每个用户，这种转换是在以用户为中心的图的对抗学习下实现的，目的是在过滤后的用户嵌入和该用户的子图结构之间混淆每个敏感特征。最后，大量的实验结果清楚地表明了我们提出的公平推荐模型的有效性。我们在 https://github.com/newlei/FairGo 上发布了源代码。

[Learning Fair Representations for Recommendation: A Graph-based Perspective | Proceedings of the Web Conference 2021 (acm.org)](https://dl.acm.org/doi/10.1145/3442381.3450015)

# ACM CCS 2023

# 30、Blink: Link Local Differential Privacy in Graph Neural Networks via Bayesian Estimation

图神经网络（GNN）由于其在各种图推理任务中学习节点嵌入的优越能力而越来越受欢迎，但对其进行训练可能会引发隐私问题。为了解决这个问题，我们提出在去中心化节点上使用链接局部差分隐私，从而能够与不受信任的服务器协作，在不泄露任何链接存在的情况下训练GNN。我们的方法在链接和图度上分别花费隐私预算，以便服务器更好地使用贝叶斯估计对图拓扑进行去噪，减轻LDP对训练后的GNN的准确性的负面影响。我们限制了推断的链接概率与真实图拓扑的平均绝对误差。然后，我们提出了两种不同的LDP机制，它们在不同的隐私设置下相互补充，其中一种机制在较低的隐私预算下估计较少的链接，以避免在不确定性较高时出现误报链接估计，而另一种机制利用更多的信息，在相对较高的隐私预算下表现更好。此外，我们提出了一种混合变体，结合了这两种策略，能够在不同的隐私预算下表现更好。广泛的实验表明，在各种隐私预算下，我们的方法在准确性方面优于现有方法。

论文链接：https://doi.org/10.1145/3576915.3623165

# 48、CryptoConcurrency: (Almost) Consensusless Asset Transfer with Shared Accounts

典型的区块链协议使用共识来确保相互不信任的用户就共享数据的操作顺序达成一致。然而，众所周知，资产转移系统是区块链最流行的应用，可以在没有共识的情况下实现。假设没有账户可以同时访问，每个账户都属于一个所有者，人们可以以纯异步、无共识的方式有效地实现资产转移系统。此外，还表明，在没有共识的情况下，使用共享账户实现资产转移是不可能的。在这篇论文中，我们提出了CryptoConcurrency，这是一种资产转移协议，允许在尽可能的情况下并行处理并发访问，而不涉及共识。更准确地说，如果给定账户上的并发转移操作不会导致超支，即可以在账户余额不低于零的情况下全部应用，它们会并行进行。否则，帐户的所有者可能必须访问外部共识对象。值得注意的是，我们避免依赖一个中心化的、普遍信任的共识机制，允许每个帐户使用自己的共识实现，只有该帐户的所有者信任它。这提供了更大的去中心化和灵活性。

# 52、DP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass

差分隐私随机梯度下降（DP-SGD）在反向传播中为梯度添加噪声，保护训练数据免受隐私泄露，特别是成员推断。它无法覆盖（推理时间）威胁，如嵌入反演和敏感属性推断。当用于微调大型预训练语言模型（LM）时，它在存储和计算方面成本很高。我们提出了DP-Forward，它直接扰乱LM的前向传递中的嵌入矩阵。它满足训练和推理数据的严格局部DP要求。为了使用最小的矩阵值噪声进行实例化，我们设计了一种分析矩阵高斯机制（aMGM），通过从矩阵高斯分布中提取可能非独立的噪声。然后，我们使用aMGM噪声研究LM不同隐藏（子）层的输出扰动。它在三个典型任务上的效用几乎达到了非私有基准，在中等隐私级别上优于DP-SGD高达7.7pp。与使用最新高速库的DP-SGD相比，它节省了3倍的时间和内存成本。它还将嵌入反演和敏感属性推理的平均成功率分别降低了88pp和41pp，而DP-SGD则失败了。

# 63、Devil in Disguise: Breaching Graph Neural Networks Privacy through Infiltration

图神经网络（GNN）已经被开发出来，用于从各种应用的图数据中挖掘有用信息，例如医疗保健、欺诈检测和社会推荐。然而，GNN为图数据的隐私攻击开辟了新的攻击面。在这篇论文中，我们提出了Infiltrator，这是一种隐私攻击，能够基于对GNN的黑盒访问来窥探节点级的私有信息。与现有的需要受害节点事先信息的工作不同，我们探索了在没有受害节点信息的情况下进行攻击的可能性。我们的想法是渗透到图中的攻击者创建的节点，与受害节点交朋友。更具体地说，我们设计了渗透方案，使对手能够推断受害节点的标签、相邻链接和敏感属性。我们通过在三个代表性GNN模型和六个真实数据集上进行广泛实验来评估Infiltrator。结果表明，Infiltrator在所有三种攻击中都能达到98%以上的攻击性能，优于基线方法。我们进一步评估了Infiltrator对图同质防御者和差分隐私模型的防御抵抗能力。


# 169、Poster: Generating Experiences for Autonomous Network Defense

强化学习（RL）为开发下一代计算机网络的防御提供了有前景的道路。希望RL不仅有助于自动化网络防御，而且RL还为防御网络找到了新的解决方案，以适应应对日益复杂的网络和威胁。尽管有希望，但将RL应用于网络安全的现有工作是在小型计算机网络上对网络防御者进行严格而狭义的问题定义训练。受研究启发，开放式学习有助于代理快速适应并推广到从未见过的任务，我们假设类似的方法可以为网络防御提供实用的RL路径。我们提供了证据来支持这一假设。实现可推广学习的一个关键方面是我们为学习代理生成经验的方法，这种方法基于任务世界，允许代理学习防御日益复杂的网络。我们证明，通过学习解决不同难度的任务，RL代理可以学习掌握相当复杂的网络防御任务。我们的初步结果表明，除了有助于掌握复杂任务的可行性外，这种类型的经验生成可能会导致更稳健的政策。总体而言，我们的研究表明，我们向学习代理提供的经验收集是实现高性能的关键方面。我们与研究界分享了我们用于（i）定义网络防御任务分布；（ii）随着代理学习更新分布；（iii）保持任务关键方面不变以保留知识的方法。我们的实验是由我们的自主网络防御高级强化学习框架（FARLAND）的第二版实现的，该框架集成了对动作表示、动态任务选择以及模拟和仿真中策略验证的支持。我们希望通过分享我们的想法和成果，促进合作和创新，以创建越来越复杂的健身房来训练网络防御者。

论文链接：https://doi.org/10.1145/3576915.3624381



# 253、Turning Privacy-preserving Mechanisms against Federated Learning

最近，研究人员成功地利用图神经网络（GNN）构建了增强的推荐系统，因为它们能够从相关实体之间的交互中学习模式。此外，之前的研究已经将联邦学习作为主要解决方案，为构建全局GNN模型提供了一种本机隐私保护机制，而无需将敏感数据收集到单个计算单元中。然而，由于对联邦客户端生成的本地模型更新的分析可能会返回与敏感的本地数据相关的信息，因此可能会出现隐私问题。为此，研究人员提出了将联邦学习与差分隐私策略和社区驱动方法相结合的解决方案，其中包括将来自邻居客户端的数据结合起来，使单个本地更新对本地敏感数据的依赖性降低。在这篇论文中，我们发现了这种配置中的一个关键安全漏洞，并设计了一种能够欺骗联邦学习最新防御的攻击。提出的攻击包括两种操作模式，第一种模式侧重于收敛抑制（对抗模式），第二种模式旨在在全球联邦模型上构建欺骗性评级注入（后门模式）。实验结果表明，我们的攻击在两种模式下都有效，在所有对抗模式测试中平均返回60%的性能损失，在93%的后门模式测试中完全有效的后门。

论文链接：https://doi.org/10.1145/3576915.3623114

# 277、martFL: Enabling Utility-Driven Data Marketplace with a Robust and Verifiable Federated Learning Architecture

机器学习模型的开发需要大量的训练数据。数据市场是交易高质量和私域数据的关键平台，这些数据在互联网上无法公开获取。然而，随着数据隐私变得越来越重要，直接交换原始数据变得不合适。联邦学习（FL）是一种分布式机器学习范式，在多方之间交换数据工具（以本地模型或梯度形式），而不直接共享原始数据。然而，我们认识到将现有的FL架构应用于构建数据市场的一些关键挑战。（i）在现有的FL架构中，数据采集器（DA）在交易之前无法私下评估不同数据提供者（DP）提交的本地模型的质量；（ii）现有FL设计中的模型聚合协议无法有效地排除恶意DP，而不会对DA（可能存在偏见）的根数据集进行“过拟合”；（iii）之前的FL设计缺乏适当的计费机制来执行DA，以根据不同DP的贡献公平分配奖励。为了解决上述挑战，我们提出了martFL，这是第一个专门用于实现安全实用驱动数据市场的联邦学习架构。从高层次来看，martFL由两个创新设计授权：（i）质量感知模型聚合协议，允许DA正确地从聚合中排除本地质量甚至有毒的本地模型，即使DA的根数据集有偏差；（ii）可验证的数据交易协议，使DA能够简洁地以零知识证明它已经根据DA承诺的权重忠实地聚合了这些本地模型。这使DP能够明确地要求与他们的权重/贡献成比例的奖励。我们实现了martFL的原型，并在各种任务上对其进行广泛评估。结果表明，martFL可以将模型精度提高25%，同时节省高达64%的数据采集成本。